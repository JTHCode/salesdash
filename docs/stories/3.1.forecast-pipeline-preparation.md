# Story 3.1: Forecast Pipeline Preparation



## Status

Done



## Story

**As a** data scientist,

**I want** a script or notebook that generates forecast outputs ahead of time,

**so that** the Streamlit app can load predictions instantly without retraining during runtime.



## Acceptance Criteria

1. Offline artifact (script or notebook) loads the static dataset, trains a moving average or ARIMA model, and writes forecast results to disk (CSV/JSON).

2. Documentation explains model choice, horizon, and evaluation metrics.

3. Forecast artifact checked into repo under a `data/processed` path with reproducible instructions.

4. Unit tests cover forecast loader function to ensure schema consistency.



## Dependencies

- Story 1.1: Streamlit Project Bootstrap established the dataset location and loader utilities the pipeline should reuse (status Done).

- Story 1.3: Data Utility & Caching Layer introduced cached helpers that set expectations for performant data access (status Done).

- Story 2.3: Regional Performance Heatmap confirmed downstream components rely on shared filter state and cached aggregations, so forecasts must align with the same data conventions (status Done).



## Project Structure Notes

- Implement forecasting logic in `src/services/forecasting_service.py`, matching the architecture's service layout for downstream reuse. [Source: architecture/source-tree.md#project-structure]

- Store generated forecast artifacts under `data/processed/` with descriptive filenames so they can be versioned and consumed by the app. [Source: docs/prd/cross-functional-requirements.md#data-requirements]

- Place automated tests in `tests/test_forecasting.py` to keep forecasting validation alongside other service tests. [Source: architecture/source-tree.md#project-structure]



## Tasks / Subtasks

- [x] Build offline forecasting pipeline module and artifact (AC: 1, 3) [Source: architecture/core-services-implementation.md#3-forecasting-service-forecasting_servicepy]

  - [x] Add `generate_forecast_artifact` (or similar) in `src/services/forecasting_service.py` that loads cleansed data via existing helpers and fits a moving average or linear regression forecast over the target metric. (AC: 1) [Source: architecture/core-services-implementation.md#1-data-loader-data_loaderpy]

  - [x] Emit forecast results (including timestamps, forecast values, confidence bounds, method metadata, and evaluation metrics) to `data/processed/forecast_sales.csv` or comparable filename. (AC: 1, 3) [Source: docs/prd/cross-functional-requirements.md#data-requirements]

  - [x] Provide a command entry point (e.g., `python -m src.services.forecasting_service`) or notebook instructions that regenerate the artifact deterministically. (AC: 3) [Source: docs/prd/cross-functional-requirements.md#data-requirements]

- [x] Document forecasting approach and reproducibility guidance (AC: 2, 3) [Source: docs/prd/cross-functional-requirements.md#data-requirements]

  - [x] Record model choice, training horizon, evaluation metric(s), and regeneration steps in project documentation (README or dedicated doc) so reviewers understand trade-offs. (AC: 2) [Source: architecture/performance-optimization.md#caching-strategy]

  - [x] Reference the generated artifact path and CLI/notebook usage so future updates follow the same reproducible process. (AC: 3) [Source: docs/prd/cross-functional-requirements.md#data-requirements]

- [x] Expose forecast loader utilities for the Streamlit app (AC: 1, 4) [Source: architecture/streamlit-application-structure.md#main-app-apppy]

  - [x] Implement a `load_forecast_results` helper returning a validated DataFrame or data class, ensuring schema columns match what forthcoming visualizations expect. (AC: 4) [Source: architecture/data-architecture.md#data-model]

  - [x] Integrate caching (`st.cache_data`) or file timestamp checks so loading precomputed forecasts stays performant. (AC: 1) [Source: architecture/performance-optimization.md#caching-strategy]

- [x] Add automated and manual validation coverage (AC: 4) [Source: architecture/testing-strategy.md#unit-tests-pytest]

  - [x] Expand `tests/test_forecasting.py` with unit tests asserting the loader enforces schema, captures metadata (method, horizon), and gracefully handles missing files. (AC: 4) [Source: architecture/source-tree.md#project-structure]

  - [x] Document manual verification steps: rerun the pipeline, inspect the generated CSV/JSON, and smoke test `streamlit run src/app.py` to ensure forecasts load without runtime training. (AC: 1-3) [Source: architecture/testing-strategy.md#local-testing]



## Dev Notes

### Previous Story Insights

- Filters and cached aggregates from Story 2.3 should remain authoritative; the forecast artifact must align with these conventions so future visualizations leverage consistent metric naming. [Source: docs/stories/2.3.regional-performance-heatmap.md#dev-notes]

- Shared metric/date state introduced in Story 2.2 informs how forecast consumers will slice data, so include corresponding metadata or aggregation grain in the exported artifact. [Source: docs/stories/2.2.time-series-visualization.md#dev-notes]



### Data Models

- Base training series on canonical fields such as `Order Date`, `Sales`, and `Total Profit/Loss`, ensuring types align with the defined schema before modeling. [Source: architecture/data-architecture.md#data-model]

- Include columns for forecast value, confidence bounds, horizon, and method so downstream components can render narratives and comparisons. [Source: architecture/core-services-implementation.md#3-forecasting-service-forecasting_servicepy]



### API Specifications

- Forecast generation occurs in-process without external APIs; reuse the embedded service pattern described in the high-level architecture. [Source: architecture/high-level-architecture.md#system-overview]

- Provide callable helpers that other modules can import rather than creating standalone executables, keeping with embedded backend design. [Source: architecture/high-level-architecture.md#system-overview]



### Component Specifications

- Structure service functions so `forecasting_viz` can consume precomputed results once Story 3.2 implements the visualization. [Source: architecture/source-tree.md#project-structure]

- Ensure returned data structures include both historical and forecast segments compatible with Plotly traces described in architecture guidance. [Source: architecture/core-services-implementation.md#3-forecasting-service-forecasting_servicepy]



### File Locations

- Place pipeline logic and loaders under `src/services/forecasting_service.py` to match the prescribed module hierarchy. [Source: architecture/source-tree.md#project-structure]

- Persist generated outputs in `data/processed/` with timestamps or semantic filenames for traceability. [Source: docs/prd/cross-functional-requirements.md#data-requirements]

- Add or extend `tests/test_forecasting.py` for automated coverage following the centralized tests directory. [Source: architecture/source-tree.md#project-structure]



### Testing Requirements

- Write pytest coverage to validate schema, value ranges, and error handling for forecast loaders. [Source: architecture/testing-strategy.md#unit-tests-pytest]

- Manual verification should follow the local testing checklist, re-running the pipeline and loading the artifact through the app. [Source: architecture/testing-strategy.md#local-testing]



### Technical Constraints

- Maintain compatibility with Python 3.11+, Streamlit 1.29+, Pandas 2.1+, NumPy 1.26+, and scikit-learn 1.4+ when selecting forecasting implementations. [Source: architecture/tech-stack.md#technology-stack]

- Respect caching TTL expectations for forecasting routines to keep refresh operations performant on Streamlit Cloud. [Source: architecture/performance-optimization.md#caching-strategy]



## Testing

- Execute `python -m src.services.forecasting_service` (or equivalent notebook run) to regenerate the artifact, then confirm file contents and metadata align with documentation. [Source: docs/prd/cross-functional-requirements.md#data-requirements]

- Run `pytest` focusing on `tests/test_forecasting.py` to validate loader behavior and schema enforcement. [Source: architecture/testing-strategy.md#unit-tests-pytest]

- Launch `streamlit run src/app.py` to verify the application can import the loader without requiring runtime training. [Source: architecture/testing-strategy.md#local-testing]



## Change Log

| Date       | Version | Description                | Author             |
|------------|---------|----------------------------|--------------------|
| 2025-10-01 | v0.1    | Initial draft of Story 3.1 | Bob (Scrum Master) |

| 2025-10-02 | v0.2    | QA remediation: moving-average forecast pipeline | James (Dev) |



## Dev Agent Record



### Agent Model Used

- GPT-5 (Codex)

### Debug Log References

- `python -m src.services.forecasting_service`
- `python -m pytest (24 passed)`

### Completion Notes List

- Implemented moving-average forecast generator with monthly aggregation and refreshed the artifact at `data/processed/forecast_sales.csv`.
- Documented the moving-average workflow in README and confirmed cached loaders via `services.load_forecast_results`.
- Updated forecasting unit tests and reran `python -m pytest` (24 passed) plus `python -m src.services.forecasting_service` to verify the artifact.

### File List

- data/processed/forecast_sales.csv
- README.md
- src/services/__init__.py
- src/services/forecasting_service.py
- tests/test_forecasting.py

## QA Results




### Review Date: 2025-10-02

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment
Implementation is clean and aligns with the architecture guidance for a reusable service: monthly aggregation is deterministic, configuration is encapsulated in `ForecastConfig`, and the cached loader enforces the artifact schema. However, Acceptance Criterion 1 explicitly calls for a moving average or ARIMA model, while the delivered generator uses a linear regression trend; this functional mismatch needs resolution with either a code change or updated acceptance criteria.

### Refactoring Performed
- None (advisory review only).

### Compliance Check
- Coding Standards: PASS � Follows service layering, dependency imports, and typing conventions from the architecture docs.
- Project Structure: PASS � Forecast pipeline lives in `src/services/forecasting_service.py` with artifact under `data/processed/` as prescribed.
- Testing Strategy: PASS � `tests/test_forecasting.py` now exercises artifact generation and loader recovery paths, and the team ran `python -m pytest` (24 passed).
- All ACs Met: CONCERNS � AC1 model requirement unmet (linear regression vs moving average/ARIMA).

### Improvements Checklist
- [ ] Align the offline model with the AC-specified moving average/ARIMA approach (or secure PO approval to revise the acceptance criteria).
- [ ] Add a small regression test that asserts the chosen forecasting method metadata matches the agreed approach once resolved.
- [ ] Consider exercising `services.load_forecast_results()` inside an integration smoke test (e.g., lightweight Streamlit session) to ensure downstream consumers can access the artifact without regeneration.

### Security Review
No sensitive data flows are introduced; the artifact contains aggregated sales metrics only. No security concerns identified.

### Performance Considerations
Pipeline executes in O(n) over the monthly series and writes a small CSV (~8 KB). Caching via `st.cache_data` keeps loader costs negligible. No performance risks.

### Files Modified During Review
None � QA review was read-only.

### Gate Status
Gate: CONCERNS -> docs/qa/gates/3.1.forecast-pipeline-preparation.yml
Risk profile: Not generated (not requested)
NFR assessment: Not generated (not requested)

### Recommended Status
[ ] Ready for Done / [x] Changes Required � See unchecked items above






### Review Date: 2025-10-02 (Re-review)

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment
Moving-average generator now matches AC1 requirements, defaulting to a 3-month rolling window with cached loading. Artifact metadata reflects the new method, and README guidance is in sync. No remaining functional gaps identified.

### Refactoring Performed
- None (verification-only pass).

### Compliance Check
- Coding Standards: PASS - Implementation adheres to service-layer conventions and typed configuration.
- Project Structure: PASS - Forecast logic, artifact location, and tests align with prescribed directories.
- Testing Strategy: PASS - Updated unit tests assert the moving-average method metadata, and `python -m pytest` shows 24 passing tests.
- All ACs Met: PASS - Offline model requirement satisfied with moving-average forecasting.

### Improvements Checklist
- [x] Align the offline model with the AC-specified moving average/ARIMA approach (or secure PO approval to revise the acceptance criteria).
- [x] Add a small regression test that asserts the chosen forecasting method metadata matches the agreed approach once resolved.
- [ ] Consider exercising `services.load_forecast_results()` inside an integration smoke test (e.g., lightweight Streamlit session) to ensure downstream consumers can access the artifact without regeneration.

### Security Review
No new risks detected; artifact remains aggregated metrics only.

### Performance Considerations
Rolling window computation and cached loader are lightweight; no concerns.

### Files Modified During Review
None � QA review confirmed developer changes.

### Gate Status
Gate: PASS -> docs/qa/gates/3.1.forecast-pipeline-preparation.yml
Risk profile: Not generated (not requested)
NFR assessment: Not generated (not requested)

### Recommended Status
[x] Ready for Done / [ ] Changes Required � Remaining suggestion is optional future hardening.
